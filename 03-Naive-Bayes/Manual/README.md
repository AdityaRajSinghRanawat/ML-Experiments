# Naive Bayes (Manual Implementation) â€” Easy Guide & Learning Module âœ…

![Naive Bayes formula](./screenshot/naive-bayes-formula.png)

## What you will learn ğŸ’¡
- The mathematical intuition behind **Naive Bayes** (manual derivation)
- How to implement Naive Bayes from scratch (stepâ€‘byâ€‘step) in `code/code.ipynb`
- How to interpret model outputs with screenshots and plots
- Exercises to practice and compare with scikitâ€‘learn implementation

---

## Prerequisites ğŸ”§
- Basic Python knowledge and familiarity with `numpy` and `pandas`
- Recommended: `matplotlib` for plots and `jupyter`/`notebook` to run the example

Quick install (if needed):

```
pip install numpy pandas matplotlib jupyter
```

---

## Files in this folder
- `code/code.ipynb` â€” manual Naive Bayes implementation and walkthrough
- `data/` â€” sample datasets used in the notebook
- `screenshot/` â€” images used below to explain formulas and outputs

---

## Short intuition â€” manual perspective ğŸ§­
Naive Bayes applies Bayes' theorem and treats features as conditionally independent given the class. A manual implementation shows how to compute prior probabilities, likelihoods (Gaussian or discrete), multiply them and pick the class with the highest posterior probability.

---

## How to run the notebook (stepâ€‘byâ€‘step) â–¶ï¸
1. Open `code/code.ipynb` in VS Code or Jupyter.
2. Run cells sequentially: load data â†’ compute priors & likelihoods â†’ implement predict() â†’ evaluate.
3. Compare manual predictions with `sklearn.naive_bayes` to validate your implementation.

---

## Notebook outputs â€” what to look for ğŸ”

### 1) Formula & derivation (visual)
Shows the Bayes theorem and the independence assumption used in calculations.

![Formula](./screenshot/naive-bayes-formula.png)

---

### 2) Probability plots / decision visualization
Visual representation of classâ€‘conditional distributions used to compute likelihoods.

![Distribution graph](./screenshot/naive-bayes-graph.jpg)

---

### 3) Numeric output / evaluation
Printed metrics and example predictions to verify the manual implementation.

![Output example](./screenshot/output.png)

---

## Key notebook sections ğŸ”¬
- Data load & EDA â€” examine feature distributions and class labels.
- Estimate priors â€” P(class) from training labels.
- Estimate likelihoods â€” for continuous features use Gaussian assumptions (mean, var).
- Prediction â€” compute posterior âˆ prior Ã— likelihood and choose argmax.
- Evaluation â€” compare accuracy and confusion matrix with scikitâ€‘learn.

---

## Exercises to practice ğŸ“
1. Implement Laplace (addâ€‘one) smoothing for categorical data.
2. Replace Gaussian likelihood with kernel density estimates and compare.
3. Compare manual implementation with `GaussianNB` on the same dataset.
4. Add crossâ€‘validation to evaluate stability.

---

## Tips & common pitfalls âš ï¸
- Beware of zero probabilities â€” use Laplace smoothing for categorical features.
- Log probabilities prevent underflow when multiplying small likelihoods.
- Check feature distributions â€” Gaussian assumption may not hold for every feature.

---

## Further reading ğŸ“š
- Wikipedia: Bayes' theorem and Naive Bayes
- scikitâ€‘learn Naive Bayes docs for reference implementations

---

## License
You may reuse or adapt this learning module for personal study. Â© Your Project
